{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TripAdvisor restaurants info scraping\n",
    "Takes a city as an argument and scrape the summary data of each restaurants of the city through the TA restaurants display pages.\n",
    "\n",
    "Curate the raw dataset generated and aggregate them into one single dataset for all cities.\n",
    "\n",
    "https://www.tripadvisor.fr/Restaurants-g274772-Krakow_Lesser_Poland_Province_Southern_Poland.html#EATERY_OVERVIEW_BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from user_agent import generate_user_agent\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variables that will be used globally through the script\n",
    "url0 = 'https://www.tripadvisor.com'\n",
    "today = datetime.datetime.now()\n",
    "today_date = str(today.year) + '/' + str(today.month) + '/' + str(today.day)\n",
    "\n",
    "#Enable display of info messages\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from the summary list of restaurants\n",
    "\n",
    "The TripAdvisor URL to scroll through the restaurants list is built as follow:\n",
    "https://www.tripadvisor.com/RestaurantSearch-g1225481-oa15, where \n",
    "- g122548 is the id of the city\n",
    "- oa30 is the variable to scroll through the pages, by incrementing by 30 to go to the next page.\n",
    "\n",
    "Restaurants are naturally sorted by descending Ranking\n",
    "The information is heterogenous: \n",
    "- all restaurants have name, id, URL\n",
    "- not all have cuisine, rank, rate, reviews\n",
    "\n",
    "### Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scraper(city):\n",
    "    query = '/TypeAheadJson?action=API&startTime='+today_date+'&uiOrigin=GEOSCOPE&source=GEOSCOPE&interleaved=true&type=geo&neighborhood_geos=true&link_type=eat&details=true&max=12&injectNeighborhoods=true&query='+city\n",
    "    url = url0 + query\n",
    "    #Query the API ad get a JSON answer readable by Python as dictionnaries objects\n",
    "    api_response = requests.get(url).json()\n",
    "    geo = api_response['results'][0]['url']  #Get the URL from the results/1st element/Url key\n",
    "    restaurants_url = url0 + geo\n",
    "    logging.info(\"Scraping {} restaurants info\".format(city))\n",
    "    print(restaurants_url)\n",
    "\n",
    "    #Prepare the scrolling requests using a URL such as\n",
    "    #https://www.tripadvisor.com/RestaurantSearch-g1225481-oa15\n",
    "    scroll_url0= 'https://www.tripadvisor.com/RestaurantSearch-'\n",
    "    b = restaurants_url.find('-')\n",
    "    e = restaurants_url.find('-', b+1)\n",
    "    city_id = restaurants_url[b+1:e]\n",
    "    \n",
    "    #Initialize the lists of parameters to scrape and the dataframe containing all data\n",
    "    inc_page = 0\n",
    "    resto_dict = {}\n",
    "    dataset = pd.DataFrame(resto_dict)\n",
    "   #columns=['Name', 'URL_TA', 'ID_TA', 'Rating', 'Ranking', 'Price Range', 'Cuisine Style', 'Number of Reviews', 'Reviews'])\n",
    "    \n",
    "    #Get the total number of pages\n",
    "    headers = {'User-Agent': generate_user_agent(device_type=\"desktop\", os=('mac', 'linux'))}\n",
    "    r = requests.get(scroll_url0+city_id, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    page_tag = soup.find_all(class_=\"pageNumbers\")[0] #tag that displays number of pages at bottom of webpage\n",
    "    a_tags = page_tag.find_all('a')  #last item of the returned list is the last page button\n",
    "    tot_pages=int(a_tags[-1].contents[0])  #integer from text content of the <a>\n",
    "    logging.info(\"{} pages to explore\".format(tot_pages))\n",
    "    \n",
    "    #Explore all the pages that display restaurants\n",
    "    for page_index in range (1, tot_pages+1):\n",
    "        \n",
    "        #URL of the current webpage\n",
    "        scroll_url = scroll_url0 + city_id + '-oa' + str(inc_page)\n",
    "        print(\"Scraping page nÂ°{}\".format(page_index))\n",
    "        print(scroll_url)\n",
    "\n",
    "        #Scrape HTML content of the current webpage using the library BeautifulSoup\n",
    "        r = requests.get(scroll_url,headers)\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "\n",
    "        #Restaurants list starts with tag <div id=\"EATERY_SEARCH_RESULTS\">\n",
    "        data_bloc = soup.find_all(attrs={\"id\": \"EATERY_SEARCH_RESULTS\"}) #contains the data bloc in a list object\n",
    "        data_bloc = data_bloc[0]  #easier to manipulate\n",
    "\n",
    "    #First restaurant of page has a particular class attribute\n",
    "        if data_bloc.find_all(class_=\"listing rebrand listingIndex-1 first\") != []:\n",
    "            resto_soup = data_bloc.find_all(class_=\"listing rebrand listingIndex-1 first\")[0]\n",
    "        else:\n",
    "            resto_soup = data_bloc.find_all(class_=\"listing rebrand first\")[0]\n",
    "\n",
    "        #Get the url, id and name of restaurants\n",
    "        url_name_tag = resto_soup.find_all(class_=\"property_title\")[0] #tag containing the data\n",
    "        #Get restaurant URL\n",
    "        resto_dict['URL_TA'] = url_name_tag.get('href')\n",
    "        #Get the restaurant ID within its URL (-dxxxxxxx-Reviews)\n",
    "        b = url_name_tag.get('href').find('-d')\n",
    "        e= url_name_tag.get('href').find('-R')\n",
    "        resto_dict['ID_TA'] = url_name_tag.get('href')[b+1:e]\n",
    "        #Get names\n",
    "        resto_dict['Name'] = url_name_tag.contents[0][1:-1]\n",
    "\n",
    "        #Get the ranking of the restaurant\n",
    "        if resto_soup.find_all(class_=\"popIndex rebrand popIndexDefault\") != []:\n",
    "            ranking_tag = resto_soup.find_all(class_=\"popIndex rebrand popIndexDefault\")[0]\n",
    "            resto_dict['Ranking'] = ranking_tag.contents[0][1:-1]\n",
    "        else:\n",
    "            resto_dict['Ranking'] = np.nan #put a NaN instead\n",
    "\n",
    "        #Get the rating of the restaurant from <span> tags\n",
    "        if resto_soup.find_all('span') != []:\n",
    "            span_tags = resto_soup.find_all('span')\n",
    "            for tag in span_tags:\n",
    "                if tag.get('alt') is not None:\n",
    "                    resto_dict['Rating'] = tag.get('alt')\n",
    "        else:\n",
    "            resto_dict['Rating'] = np.nan\n",
    "\n",
    "        #Information from <div class=\"cuisines\">  \n",
    "        #!! some resaurants don't have pricerange nor cuisine styles, instead <div class=\"cuisine_margin\">\n",
    "        cuisines_tags = resto_soup.find_all(class_=\"cuisines\") #1 element of the list is 1 restaurant\n",
    "        if resto_soup.find_all(class_=\"cuisines\") != []:\n",
    "            for item in cuisines_tags:\n",
    "                #Get price range from <span class=\"item price\">\n",
    "                if item.find(class_=\"item price\") is not None:\n",
    "                    price_range = item.find(class_=\"item price\") #unique tag with price range\n",
    "                    resto_dict['Price Range'] = price_range.contents[0]\n",
    "                else:\n",
    "                     resto_dict['Price Range'] = np.nan\n",
    "                #Get cuisine styles from <span class=\"item cuisine\"> tags (several/restaurant)\n",
    "                if item.find_all(class_=\"item cuisine\") != []:\n",
    "                    cuisines = item.find_all(class_=\"item cuisine\")  # list of <a> tags with the cuisine style as text\n",
    "                    resto_dict['Cuisine Style'] = [tag.contents[0] for tag in cuisines]\n",
    "                else:\n",
    "                    resto_dict['Cuisine Style'] = np.nan\n",
    "\n",
    "        #Get number of reviews\n",
    "        if resto_soup.find_all(class_=\"reviewCount\") != []:\n",
    "            numb_tag = resto_soup.find_all(class_=\"reviewCount\")[0]\n",
    "            resto_dict['Number of Reviews'] = numb_tag.find('a').contents[0][1:-9]\n",
    "        else:\n",
    "            resto_dict['Number of Reviews'] = np.nan\n",
    "            \n",
    "        #Get 2 reviews (text+date) from <ul class=\"review_stubs review_snippets rebrand\"> and <li> tags within\n",
    "        ul_tags = resto_soup.find_all(class_=\"review_stubs review_snippets rebrand\")\n",
    "        if ul_tags != []:\n",
    "            for reviews_set in ul_tags:\n",
    "                rev_texts = reviews_set.find_all(dir=\"ltr\")\n",
    "                rev_dates = reviews_set.find_all(class_=\"date\")\n",
    "                resto_dict['Reviews'] = [[tag.find('a').contents[0] for tag in rev_texts], #text is in a <a> tag\n",
    "                                          [tag.contents[0] for tag in rev_dates]]\n",
    "        else:\n",
    "            resto_dict['Reviews'] = np.nan\n",
    "            \n",
    "        #Append the dataset\n",
    "        dataset = pd.concat([dataset, pd.DataFrame([resto_dict])])\n",
    "            \n",
    "    #For the rest of the list from 2 to 30:\n",
    "        try:\n",
    "            inc_rest = 0\n",
    "            for i in range (2, 31):\n",
    "                resto_dict = {}\n",
    "                resto_bloc_id = \"listing rebrand listingIndex-\" + str(i)\n",
    "                if data_bloc.find_all(class_=resto_bloc_id) != []:\n",
    "                    resto_soup = data_bloc.find_all(class_=resto_bloc_id)[0] #Bloc for one restaurant\n",
    "\n",
    "                    #Get the url, id and name of restaurants\n",
    "                    url_name_tag = resto_soup.find_all(class_=\"property_title\")[0] #tag containing the data\n",
    "                    #Get restaurant URL\n",
    "                    resto_dict['URL_TA'] = url_name_tag.get('href')\n",
    "                    #Get the restaurant ID within its URL (-dxxxxxxx-Reviews)\n",
    "                    b = url_name_tag.get('href').find('-d')\n",
    "                    e= url_name_tag.get('href').find('-R')\n",
    "                    resto_dict['ID_TA'] = url_name_tag.get('href')[b+1:e]\n",
    "                    #Get names\n",
    "                    resto_dict['Name'] = url_name_tag.contents[0][1:-1]\n",
    "\n",
    "                    #Get the ranking of the restaurant\n",
    "                    if resto_soup.find_all(class_=\"popIndex rebrand popIndexDefault\") != []:\n",
    "                        ranking_tag = resto_soup.find_all(class_=\"popIndex rebrand popIndexDefault\")[0]\n",
    "                        resto_dict['Ranking'] = ranking_tag.contents[0][1:-1]\n",
    "                    else:\n",
    "                        resto_dict['Ranking'] = np.nan\n",
    "\n",
    "                    #Get the rating of the restaurant from <span> tags\n",
    "                    span_tags = resto_soup.find_all('span')\n",
    "                    if resto_soup.find_all('span') != []:\n",
    "                        for tag in span_tags:\n",
    "                            if tag.get('alt') is not None:\n",
    "                                resto_dict['Rating'] = tag.get('alt')\n",
    "                    else:\n",
    "                        resto_dict['Rating'] = np.nan\n",
    "\n",
    "                    #Information from <div class=\"cuisines\">  \n",
    "                    #!! some resaurants don't have pricerange nor cuisine styles, instead <div class=\"cuisine_margin\">\n",
    "                    cuisines_tags = resto_soup.find_all(class_=\"cuisines\") #1 element of the list is 1 restaurant\n",
    "                    if resto_soup.find_all(class_=\"cuisines\") != []:\n",
    "                        for item in cuisines_tags:\n",
    "                            #Get price range from <span class=\"item price\">\n",
    "                            if item.find(class_=\"item price\") is not None:\n",
    "                                price_range = item.find(class_=\"item price\") #unique tag with price range\n",
    "                                resto_dict['Price Range'] = price_range.contents[0]\n",
    "                            else:\n",
    "                                resto_dict['Price Range'] = np.nan\n",
    "                            #Get cuisine styles from <span class=\"item cuisine\"> tags (several/restaurant)\n",
    "                            if item.find_all(class_=\"item cuisine\") != []:\n",
    "                                cuisines = item.find_all(class_=\"item cuisine\")  # list of <a> tags with the cuisine style as text\n",
    "                                resto_dict['Cuisine Style'] = [tag.contents[0] for tag in cuisines]\n",
    "                            else: \n",
    "                                resto_dict['Cuisine Style'] = np.nan\n",
    "\n",
    "                    #Get number of reviews\n",
    "                    if resto_soup.find_all(class_=\"reviewCount\") != []:\n",
    "                        numb_tag = resto_soup.find_all(class_=\"reviewCount\")[0]\n",
    "                        resto_dict['Number of Reviews'] = numb_tag.find('a').contents[0][1:-9]\n",
    "                    else:\n",
    "                        resto_dict['Number of Reviews'] = np.nan\n",
    "\n",
    "                    #Get 2 reviews (text+date) from <ul class=\"review_stubs review_snippets rebrand\"> and <li> tags within\n",
    "                    ul_tags = resto_soup.find_all(class_=\"review_stubs review_snippets rebrand\")\n",
    "                    if resto_soup.find_all(class_=\"review_stubs review_snippets rebrand\") != []:\n",
    "                        for reviews_set in ul_tags:\n",
    "                            rev_texts = reviews_set.find_all(dir=\"ltr\")\n",
    "                            rev_dates = reviews_set.find_all(class_=\"date\")\n",
    "                            #Able to pick up empty displayed review \"\" (St morris Argentijns, Amsterdam)\n",
    "                            resto_dict['Reviews'] = [[tag.find('a').contents[0] if tag.find('a').contents != [] else np.nan for tag in rev_texts], #text is in a <a> tag\n",
    "                                                  [tag.contents[0] for tag in rev_dates]]\n",
    "                    else:\n",
    "                        resto_dict['Reviews'] = np.nan\n",
    "\n",
    "                    #Append the dataset\n",
    "                    dataset = pd.concat([dataset, pd.DataFrame([resto_dict])])\n",
    "\n",
    "                else: #tag of restaurant is instead \"listing rebrand\"\n",
    "                    resto_soup = data_bloc.find_all(class_=\"listing rebrand\")[inc_rest]\n",
    "\n",
    "                    #Get the url, id and name of restaurants\n",
    "                    url_name_tag = resto_soup.find_all(class_=\"property_title\")[0] #tag containing the data\n",
    "                    #Get restaurant URL\n",
    "                    resto_dict['URL_TA'] = url_name_tag.get('href')\n",
    "                    #Get the restaurant ID within its URL (-dxxxxxxx-Reviews)\n",
    "                    b = url_name_tag.get('href').find('-d')\n",
    "                    e= url_name_tag.get('href').find('-R')\n",
    "                    resto_dict['ID_TA'] = url_name_tag.get('href')[b+1:e]\n",
    "                    #Get names\n",
    "                    resto_dict['Name'] = url_name_tag.contents[0][1:-1]\n",
    "\n",
    "                    #Get the ranking of the restaurant\n",
    "                    if resto_soup.find_all(class_=\"popIndex rebrand popIndexDefault\") != []:\n",
    "                        ranking_tag = resto_soup.find_all(class_=\"popIndex rebrand popIndexDefault\")[0]\n",
    "                        resto_dict['Ranking'] = ranking_tag.contents[0][1:-1]\n",
    "                    else:\n",
    "                        resto_dict['Ranking'] = np.nan\n",
    "\n",
    "                    #Get the rating of the restaurant from <span> tags\n",
    "                    span_tags = resto_soup.find_all('span')\n",
    "                    if resto_soup.find_all('span') != []:\n",
    "                        for tag in span_tags:\n",
    "                            if tag.get('alt') is not None:\n",
    "                                resto_dict['Rating'] = tag.get('alt')\n",
    "                    else:\n",
    "                        resto_dict['Rating'] = np.nan\n",
    "\n",
    "                    #Information from <div class=\"cuisines\">  \n",
    "                    #!! some resaurants don't have pricerange nor cuisine styles, instead <div class=\"cuisine_margin\">\n",
    "                    cuisines_tags = resto_soup.find_all(class_=\"cuisines\") #1 element of the list is 1 restaurant\n",
    "                    if resto_soup.find_all(class_=\"cuisines\") != []:\n",
    "                        for item in cuisines_tags:\n",
    "                            #Get price range from <span class=\"item price\">\n",
    "                            if item.find(class_=\"item price\") is not None:\n",
    "                                price_range = item.find(class_=\"item price\") #unique tag with price range\n",
    "                                resto_dict['Price Range'] = price_range.contents[0]\n",
    "                            else:\n",
    "                                resto_dict['Price Range'] = np.nan\n",
    "                            #Get cuisine styles from <span class=\"item cuisine\"> tags (several/restaurant)\n",
    "                            if item.find_all(class_=\"item cuisine\") != []:\n",
    "                                cuisines = item.find_all(class_=\"item cuisine\")  # list of <a> tags with the cuisine style as text\n",
    "                                resto_dict['Cuisine Style'] = [tag.contents[0] for tag in cuisines]\n",
    "                            else: \n",
    "                                resto_dict['Cuisine Style'] = np.nan\n",
    "\n",
    "                    #Get number of reviews\n",
    "                    if resto_soup.find_all(class_=\"reviewCount\") != []:\n",
    "                        numb_tag = resto_soup.find_all(class_=\"reviewCount\")[0]\n",
    "                        resto_dict['Number of Reviews'] = numb_tag.find('a').contents[0][1:-9]\n",
    "                    else:\n",
    "                        resto_dict['Number of Reviews'] = np.nan\n",
    "\n",
    "                    #Get 2 reviews (text+date) from <ul class=\"review_stubs review_snippets rebrand\"> and <li> tags within\n",
    "                    ul_tags = resto_soup.find_all(class_=\"review_stubs review_snippets rebrand\")\n",
    "                    if resto_soup.find_all(class_=\"review_stubs review_snippets rebrand\") != []:\n",
    "                        for reviews_set in ul_tags:\n",
    "                            rev_texts = reviews_set.find_all(dir=\"ltr\")\n",
    "                            rev_dates = reviews_set.find_all(class_=\"date\")\n",
    "                            #Able to pick up empty displayed review \"\" (St morris Argentijns, Amsterdam)\n",
    "                            resto_dict['Reviews'] = [[tag.find('a').contents[0] if tag.find('a').contents != [] else np.nan for tag in rev_texts], #text\n",
    "                                                  [tag.contents[0] for tag in rev_dates]]\n",
    "                    else:\n",
    "                        resto_dict['Reviews'] = np.nan\n",
    "\n",
    "                    #Append the dataset\n",
    "                    dataset = pd.concat([dataset, pd.DataFrame([resto_dict])])\n",
    "\n",
    "                    inc_rest += 1\n",
    "\n",
    "            #Increment to next page to display the next 30 restaurants\n",
    "            inc_page += 30\n",
    "    \n",
    "        #End scrolling when no more restaurants when not able to find other restaurant bloc\n",
    "        except IndexError:\n",
    "            logging.info(\"Last restaurant reached\")\n",
    "            break\n",
    "    \n",
    "    #Save dataframe as csv file\n",
    "    dataset.to_csv(city + '_TA_restaurants_raw.csv', sep=',', encoding=\"utf-8\")\n",
    "    print(\"File created in current directory: {}_TA_restaurants_raw.csv\".format(city))\n",
    "\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a middle-size city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Test\n",
    "scraper('Lyon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape euro capitals for restaurants data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the scraper to get data from the euro capitals\n",
    "euro_capitals = ['Paris', 'London', 'Budapest', 'Madrid', 'Lisbon', 'Berlin', 'Rome', \n",
    "            'Athens', 'Vienna', 'Warsaw', 'Ljubljana', 'Dublin',\n",
    "                 'Bruxelles', 'Prague', 'Amsterdam', 'Luxembourg', 'Bratislava',\n",
    "                'Copenhagen', 'Oslo', 'Helsinki', 'Stockholm', 'Geneva']\n",
    "for city in euro_capitals:\n",
    "    scraper(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the scraper to get data from the euro other main cities\n",
    "euro_second = ['Lyon', 'Barcelona', 'Oporto', 'Milan', 'Munich', 'Edinburgh', 'Krakow', 'Zurich', 'Hamburg']\n",
    "for city in euro_second :\n",
    "    scraper(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation\n",
    "## Exploration of raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the datasets obtained from the scraper\n",
    "raw_csv_files = glob2.glob('*raw.csv')\n",
    "print (raw_csv_files)\n",
    "print(\"{} files in the directory\".format(len(raw_csv_files)))\n",
    "for file in raw_csv_files:\n",
    "    print('\\n' + file)\n",
    "    dataset = pd.read_csv(file, sep=',',  encoding=\"utf-8\", index_col=0)\n",
    "    print(dataset.info())\n",
    "    print(dataset.head())\n",
    "    print(dataset.tail())\n",
    "\n",
    "    #Count the unique values in Price range and rating columns\n",
    "    print(dataset['Price Range'].value_counts(dropna=False))\n",
    "    print(dataset['Rating'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems to fix in the raws datasets:\n",
    "- All data are object type \n",
    "    - the price range and rate are categorical variables, taking 3 and 9 values (NaN not included)\n",
    "    - Rate, rank, number of reviews as numerical data\n",
    "- No row Index, can used instead rank\n",
    "- rearrange orger of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Curation\n",
    "\n",
    "Cures raw datasets to:\n",
    "- have numerical useful values for number reviews, rate, rank\n",
    "- have price range as a categorical type\n",
    "- lists ready to be parsed\n",
    "- rows in order and first empty row deleted\n",
    "- add a column with city name for further concatenation\n",
    "\n",
    "==> Creates a global dataset with concatenated curated dataset\n",
    "\n",
    "Using the rank of the restaurant as the index is not possible as there are plenty NaN values by the end of the table for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curates all the raw.csv files in the current director and create a new curated & aggregated dataset    \n",
    "raw_csv_files = glob2.glob('*raw.csv')\n",
    "print(\"{} cities raw datasets available\".format(len(raw_csv_files)))\n",
    "curated_dataset = pd.DataFrame()\n",
    "\n",
    "#Curates all the raw files from scraper\n",
    "for file in raw_csv_files:\n",
    "    city = file[:file.find('_')]\n",
    "    print(city + ': Curating ' + file)\n",
    "    dataset = pd.read_csv(file, sep=',',  encoding=\"utf-8\")\n",
    "    \n",
    "    #broadcast the city name in the dataset\n",
    "    dataset['City'] = city\n",
    "    \n",
    "    #Rating column into numerical data by slicing ' of 5 bubbles'\n",
    "    dataset['Rating'] = dataset['Rating'].apply(lambda x: str(x)[:-13])\n",
    "    dataset['Rating'] = pd.to_numeric(dataset['Rating'], errors='ignore')\n",
    "    \n",
    "    #Ranking column into numerical by slicing '#' and ' of xx restaurants in city'\n",
    "    dataset['Ranking'] = dataset['Ranking'].apply(lambda x: str(x)[1:(str(x).find(' of'))].replace(',',''))\n",
    "    dataset['Ranking'] = pd.to_numeric(dataset['Ranking'], errors='coerce')\n",
    "    \n",
    "    #Number of reviews column into numerical\n",
    "    dataset['Number of Reviews'] =  dataset['Number of Reviews'].apply(lambda x: str(x).replace(',',''))\n",
    "    dataset['Number of Reviews'] = pd.to_numeric(dataset['Number of Reviews'], errors='coerce')\n",
    "    \n",
    "    #Price range as categorical type:\n",
    "    dataset['Price Range'] = dataset['Price Range'].astype('category')\n",
    "    \n",
    "    #Re-order columns\n",
    "    dataset = dataset[['Name', 'City', 'Cuisine Style', 'Ranking', 'Rating', \n",
    "                       'Price Range', 'Number of Reviews', 'Reviews', 'URL_TA', 'ID_TA']]\n",
    "    \n",
    "    #Create a curated csv file\n",
    "    dataset.to_csv(file.replace('raw', 'curated'),  sep=',', encoding=\"utf-8\")\n",
    "    print('Curated dataset created')\n",
    "\n",
    "    #Append the curated dataset with the new curated dataset of the current city\n",
    "    curated_dataset = pd.concat([curated_dataset, dataset])\n",
    "    \n",
    "#Categorize the city column data\n",
    "curated_dataset['City'] = curated_dataset['City'].astype('category')\n",
    "\n",
    "#Create the curated csv file\n",
    "curated_dataset.to_csv('TA_restaurants_curated.csv', sep=',', encoding=\"utf-8\")\n",
    "logging.info('Curated dataset created')\n",
    "print(curated_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['Rating'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
